{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "0cb8d70b",
      "metadata": {
        "id": "0cb8d70b"
      },
      "source": [
        "# The notebook is dedicated to fine-tuning models using only the 'Drag Race' transcript data."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c167888",
      "metadata": {
        "id": "7c167888"
      },
      "source": [
        "Importing required libraries and modules."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "1df4654e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1df4654e",
        "outputId": "12a6ab20-9086-4f8a-96b3-27134e967fa8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.32.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.16.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "445b23e3",
      "metadata": {
        "id": "445b23e3"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "d27b657c",
      "metadata": {
        "id": "d27b657c"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('Notebook_6_7_dragrace_transcript_wrongpreds.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5dc0825d",
      "metadata": {
        "id": "5dc0825d"
      },
      "source": [
        "Adding ground truth hate_label column to drag race transcript data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "1bfd0fc0",
      "metadata": {
        "id": "1bfd0fc0"
      },
      "outputs": [],
      "source": [
        "data['hate_label'] = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "136bf689",
      "metadata": {
        "id": "136bf689"
      },
      "source": [
        "Importing required libraries and modules."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "3db9d7a3",
      "metadata": {
        "id": "3db9d7a3"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\n",
        "\n",
        "# 1. Load the pre-trained model and tokenizer\n",
        "model_name = \"facebook/roberta-hate-speech-dynabench-r4-target\"\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "93f9341a",
      "metadata": {
        "id": "93f9341a"
      },
      "source": [
        "Preprocessing the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "3208a8d8",
      "metadata": {
        "id": "3208a8d8"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import string\n",
        "\n",
        "def preprocess_sentence(sentence):\n",
        "  # no lowercasing or punctuation removal as assumed to carry semantic information\n",
        "    sentence = re.sub(r'\\\\n', ' ', sentence)\n",
        "    sentence = re.sub(r'\\s+', ' ', sentence).strip()\n",
        "    return sentence\n",
        "\n",
        "# Apply the preprocess_sentence function to the 'sentences' column\n",
        "data['sentences'] = data['sentences'].apply(preprocess_sentence)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "817a3b20",
      "metadata": {
        "id": "817a3b20"
      },
      "source": [
        "Preparing the texts and labels for classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "2542aa02",
      "metadata": {
        "id": "2542aa02"
      },
      "outputs": [],
      "source": [
        "train_texts = data['sentences'].to_list()\n",
        "train_labels = data['hate_label'].to_list()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cec97f12",
      "metadata": {
        "id": "cec97f12"
      },
      "source": [
        "Loading the dataset from Vidgen et al. (2021) from which the test dataset will be taken for evaluating the fine-tuned model's performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "48b18155",
      "metadata": {
        "id": "48b18155"
      },
      "outputs": [],
      "source": [
        "dataset2 = pd.read_csv('Notebook_7_Dynamically Generated Hate Dataset v0.2.3.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "6d13258c",
      "metadata": {
        "id": "6d13258c"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "encoder = LabelEncoder()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "6b672a57",
      "metadata": {
        "id": "6b672a57"
      },
      "outputs": [],
      "source": [
        "# Encode the original labels\n",
        "encoded_labels = encoder.fit_transform(dataset2['label'])\n",
        "\n",
        "# Create a mapping dictionary to reverse the labels\n",
        "mapping = {'nothate': 0, 'hate': 1}\n",
        "\n",
        "# Apply the mapping to reverse the labels\n",
        "dataset2['Numeric_label'] = [mapping[label] for label in dataset2['label']]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "108244f3",
      "metadata": {
        "id": "108244f3"
      },
      "source": [
        "Creating variables to store the test dataset texts and labels from Vidgen et al. (2021)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "cd3567e4",
      "metadata": {
        "id": "cd3567e4"
      },
      "outputs": [],
      "source": [
        "dataset_test = dataset2.loc[dataset2['split'] == 'test']\n",
        "test_texts = dataset_test['text'][dataset_test['round.base'] == 4].values.tolist()\n",
        "test_labels = dataset_test['Numeric_label'][dataset_test['round.base'] == 4].values.tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d27f7a9",
      "metadata": {
        "id": "0d27f7a9"
      },
      "source": [
        "Creating a custom Dataloader which will be used to process the train and test datasets in the pretrained RoBERTa model."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a custom dataset class for hate speech detection using PyTorch\n",
        "class HateSpeechDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    # Initialize the dataset object\n",
        "    def __init__(self, texts, labels, tokenizer, max_len):\n",
        "        # Store the list of textual samples\n",
        "        self.texts = texts\n",
        "        # Store the list of labels corresponding to each text sample\n",
        "        self.labels = labels\n",
        "        # Store the tokenizer instance which will convert text to tokens\n",
        "        self.tokenizer = tokenizer\n",
        "        # Store the maximum token length for sequences\n",
        "        self.max_len = max_len\n",
        "\n",
        "    # Return the total number of samples in the dataset\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    # Fetch and return a single data sample given its index\n",
        "    def __getitem__(self, item):\n",
        "        # Retrieve the text and its corresponding label using the provided index\n",
        "        text = self.texts[item]\n",
        "        label = self.labels[item]\n",
        "\n",
        "        # Tokenize the text using the provided tokenizer\n",
        "        # This converts the text to a format suitable for model input\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,   # Add special tokens like [CLS], [SEP]\n",
        "            max_length=self.max_len,   # Ensure the sequence doesn't exceed the max length\n",
        "            padding='max_length',      # Pad short sequences to the max length\n",
        "            truncation=True,           # Truncate sequences exceeding the max length\n",
        "            return_tensors='pt'        # Return data as PyTorch tensors\n",
        "        )\n",
        "\n",
        "        # Return a dictionary containing the tokenized data and the label\n",
        "        return {\n",
        "            # The token IDs of the text\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            # A mask to indicate real tokens (1) vs padded tokens (0)\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            # The corresponding label of the text sample\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }\n"
      ],
      "metadata": {
        "id": "se4g1Xs9yEqd"
      },
      "id": "se4g1Xs9yEqd",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "06a47387",
      "metadata": {
        "id": "06a47387"
      },
      "source": [
        "Creating the train and test datasets using the custom dataloader function and defining the maximum length of texts to be tokenized."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "d83f0be4",
      "metadata": {
        "id": "d83f0be4"
      },
      "outputs": [],
      "source": [
        "max_len = 128\n",
        "train_dataset = HateSpeechDataset(train_texts, train_labels, tokenizer, max_len)\n",
        "test_dataset = HateSpeechDataset(test_texts, test_labels, tokenizer, max_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "67f4f1a3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "67f4f1a3",
        "outputId": "3870c33f-25a8-4704-91fc-e2fe43091349"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.22.0)\n",
            "Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.32.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.0.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.16.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.31.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.3.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers[torch]) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers[torch]) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (3.27.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (16.0.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install accelerate -U transformers[torch]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "270bf738",
      "metadata": {
        "id": "270bf738"
      },
      "source": [
        "The training loop for the fine-tuning process using the Huggingface Trainer calss. and the evaluation of the fine-tuned model using the test dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "cabe62cf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "id": "cabe62cf",
        "outputId": "899a6fff-c18e-4f89-fc0c-0130aad183df"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='195' max='195' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [195/195 01:08, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=195, training_loss=0.5042786035782252, metrics={'train_runtime': 72.5781, 'train_samples_per_second': 42.658, 'train_steps_per_second': 2.687, 'total_flos': 203647956848640.0, 'train_loss': 0.5042786035782252, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from transformers import EvalPrediction\n",
        "import numpy as np\n",
        "\n",
        "# Define evaluation metrics function to include the accuracy and F1 scores using the prediction classes from the model\n",
        "def compute_metrics(p: EvalPrediction):\n",
        "    preds = np.argmax(p.predictions, axis=1)\n",
        "    return {\n",
        "        'accuracy': accuracy_score(p.label_ids, preds),\n",
        "        'f1': f1_score(p.label_ids, preds, average='weighted')\n",
        "    }\n",
        "\n",
        "# Step 5: Fine-tuning the Model using the Trainer class\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "from transformers import EarlyStoppingCallback\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=500,\n",
        "    evaluation_strategy='steps',\n",
        "    eval_steps=500,\n",
        "    load_best_model_at_end=True,  # Set load_best_model_at_end to True\n",
        "    # Remove 'early_stopping_patience' from here\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Evaluate on Test Set from Vidgen et al\n",
        "eval_result = trainer.evaluate(test_dataset)\n",
        "print(f\"Test Accuracy: {eval_result['eval_accuracy']:.4f}, Test F1: {eval_result['eval_f1']:.4f}\")\n"
      ],
      "metadata": {
        "id": "PKnWAoQ5WQlt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "3e87d964-6c10-4580-bc03-b44701d226f8"
      },
      "id": "PKnWAoQ5WQlt",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='64' max='64' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [64/64 00:07]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.5419, Test F1: 0.4005\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset3 = pd.read_csv('Notebook_7_drag_transcript_testset.csv')"
      ],
      "metadata": {
        "id": "FbDKJxd3Wii4"
      },
      "id": "FbDKJxd3Wii4",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocessing the test dataset"
      ],
      "metadata": {
        "id": "N_z_T_CzZ69r"
      },
      "id": "N_z_T_CzZ69r"
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "\n",
        "def preprocess_sentence(sentence):\n",
        "  # no lowercasing or punctuation removal as assumed to carry semantic information\n",
        "    sentence = re.sub(r'\\\\n', ' ', sentence)\n",
        "    sentence = re.sub(r'\\s+', ' ', sentence).strip()\n",
        "    return sentence\n",
        "\n",
        "# Apply the preprocess_sentence function to the 'sentences' column\n",
        "dataset3['Sentence'] = dataset3['Sentence'].apply(preprocess_sentence)\n"
      ],
      "metadata": {
        "id": "GWsFxqRJZ6RI"
      },
      "id": "GWsFxqRJZ6RI",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_texts2 = dataset3['Sentence'].values.tolist()\n",
        "test_labels2 = dataset3['hate_label'].values.tolist()"
      ],
      "metadata": {
        "id": "1R69ghTtW3Qg"
      },
      "id": "1R69ghTtW3Qg",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset2 = HateSpeechDataset(test_texts2, test_labels2, tokenizer, max_len)"
      ],
      "metadata": {
        "id": "8Hr6U-vJXJQn"
      },
      "id": "8Hr6U-vJXJQn",
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7: Evaluate on Test Set from Drag_race_testset\n",
        "eval_result2 = trainer.evaluate(test_dataset2)\n",
        "print(f\"Test Accuracy: {eval_result2['eval_accuracy']:.4f}, Test F1: {eval_result2['eval_f1']:.4f}\")\n"
      ],
      "metadata": {
        "id": "6gjzAQJbWRUL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "90566678-217d-48c1-dc4c-1bc5ab687425"
      },
      "id": "6gjzAQJbWRUL",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='235' max='64' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [64/64 01:04]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 1.0000, Test F1: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Vd_-fyjWa1a1"
      },
      "id": "Vd_-fyjWa1a1",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}