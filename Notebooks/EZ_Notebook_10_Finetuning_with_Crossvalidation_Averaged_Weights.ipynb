{"cells":[{"cell_type":"markdown","id":"f48190df","metadata":{"id":"f48190df"},"source":["# This notebook focuses on fine-tuning the Vidgen et al. (2021) model using cross-validation and averaging weights."]},{"cell_type":"markdown","id":"eceb2eb9","metadata":{"id":"eceb2eb9"},"source":["Importing required libraries and modules."]},{"cell_type":"code","execution_count":null,"id":"11a83ee0","metadata":{"id":"11a83ee0"},"outputs":[],"source":["import pandas as pd\n","from tqdm import tqdm\n","from sklearn.model_selection import train_test_split\n","import torch"]},{"cell_type":"markdown","id":"8b209acb","metadata":{"id":"8b209acb"},"source":["Loading data."]},{"cell_type":"code","execution_count":null,"id":"942d4f49","metadata":{"id":"942d4f49"},"outputs":[],"source":["data = pd.read_csv('Notebook_8_9_10_fine_tune_final.csv')"]},{"cell_type":"code","execution_count":null,"id":"e81a7c32","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6412,"status":"ok","timestamp":1692365653571,"user":{"displayName":"Eszter Zsisku","userId":"05092016175184633832"},"user_tz":-60},"id":"e81a7c32","outputId":"8a0f496d-a94d-47b5-c75e-1a9e182b4254"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.31.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.16.4)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.2)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n"]}],"source":["!pip install transformers"]},{"cell_type":"markdown","id":"44d72803","metadata":{"id":"44d72803"},"source":["Importing required libraries and modules and Vidgen et al's (2021) model."]},{"cell_type":"code","execution_count":null,"id":"eb4fb1e6","metadata":{"id":"eb4fb1e6"},"outputs":[],"source":["from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\n","\n","# 1. Load the pre-trained model and tokenizer\n","model_name = \"facebook/roberta-hate-speech-dynabench-r4-target\"\n","model = AutoModelForSequenceClassification.from_pretrained(model_name)\n","tokenizer = AutoTokenizer.from_pretrained(model_name)"]},{"cell_type":"markdown","id":"27b0e65e","metadata":{"id":"27b0e65e"},"source":["Logging into Huggingface to allow for fine-tune model to be uploaded onto account."]},{"cell_type":"code","execution_count":null,"id":"e278dac0","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16731,"status":"ok","timestamp":1692365675900,"user":{"displayName":"Eszter Zsisku","userId":"05092016175184633832"},"user_tz":-60},"id":"e278dac0","outputId":"79167378-015c-48f0-ede2-9448fe229732"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.16.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.12.2)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2023.6.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.31.0)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.7.1)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (23.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.2.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2023.7.22)\n","\n","    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n","    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n","    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n","    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n","    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n","    \n","    A token is already saved on your machine. Run `huggingface-cli whoami` to get more information or `huggingface-cli logout` if you want to log out.\n","    Setting a new token will erase the existing one.\n","    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n","Token: \n","Add token as git credential? (Y/n) n\n","Token is valid (permission: write).\n","Your token has been saved to /root/.cache/huggingface/token\n","Login successful\n"]}],"source":["!pip install huggingface_hub\n","!huggingface-cli login"]},{"cell_type":"code","execution_count":null,"id":"8255eda4","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":827,"status":"ok","timestamp":1692365676703,"user":{"displayName":"Eszter Zsisku","userId":"05092016175184633832"},"user_tz":-60},"id":"8255eda4","outputId":"5da90fd2-ff3f-4466-e0c3-54d1faf46a37"},"outputs":[{"name":"stdout","output_type":"stream","text":["EZiisk\n"]}],"source":["!huggingface-cli whoami"]},{"cell_type":"code","execution_count":null,"id":"7dd46bf9","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5621,"status":"ok","timestamp":1692365682308,"user":{"displayName":"Eszter Zsisku","userId":"05092016175184633832"},"user_tz":-60},"id":"7dd46bf9","outputId":"fd80ca35-f762-439d-9f37-f421770b86d2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.21.0)\n","Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.31.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.1)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n","Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.0.1+cu118)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.12.2)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.16.4)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.31.0)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.13.3)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.3.2)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.66.1)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers[torch]) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers[torch]) (4.7.1)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (3.27.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (16.0.6)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.2.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2023.7.22)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"]}],"source":["!pip install accelerate -U transformers[torch]"]},{"cell_type":"markdown","id":"28592c84","metadata":{"id":"28592c84"},"source":["Importing required libraries and modules and preprocessing sentences."]},{"cell_type":"code","execution_count":null,"id":"76194fb8","metadata":{"id":"76194fb8"},"outputs":[],"source":["import re\n","import string\n","\n","def preprocess_sentence(sentence):\n","  # no lowercasing or punctuation removal as assumed to carry semantic information\n","    sentence = re.sub(r'\\\\n', ' ', sentence)\n","    sentence = re.sub(r'\\s+', ' ', sentence).strip()\n","    return sentence\n","\n","# Apply the preprocess_sentence function to the 'sentences' column\n","data['sentences'] = data['sentences'].apply(preprocess_sentence)\n"]},{"cell_type":"markdown","id":"7926e428","metadata":{"id":"7926e428"},"source":["Creating the train/validation/test split by stratifying the data using the gold label column."]},{"cell_type":"code","execution_count":null,"id":"0f68c5e2","metadata":{"id":"0f68c5e2"},"outputs":[],"source":["# Initialize lists to store train, validate, and test data\n","train_data, val_data, test_data = [], [], []\n","\n","# Group data by \"gold_label\" and create lists of texts and labels for each group\n","grouped_data = data.groupby('gold_label')\n","grouped_texts = [group['sentences'].tolist() for _, group in grouped_data]\n","grouped_labels = [group['hate_label'].tolist() for _, group in grouped_data]\n","\n","# Split each group into train (70%), validate (15%), and test (15%) sets\n","for texts, labels in zip(grouped_texts, grouped_labels):\n","    # Split into train (85%) and test (15%)\n","    train_texts_group, test_texts_group, train_labels_group, test_labels_group = train_test_split(\n","        texts, labels, test_size=0.15, stratify=labels, random_state=42\n","    )\n","\n","    # Split train set into train (82.35%) and validate (17.65%) to achieve a 70-15-15 split\n","    train_texts_group, val_texts_group, train_labels_group, val_labels_group = train_test_split(\n","        train_texts_group, train_labels_group, test_size=0.1765, stratify=train_labels_group, random_state=42\n","    )\n","\n","    train_data.extend(list(zip(train_texts_group, train_labels_group)))\n","    val_data.extend(list(zip(val_texts_group, val_labels_group)))\n","    test_data.extend(list(zip(test_texts_group, test_labels_group)))\n","\n","# Separate the train, validate, and test texts and labels\n","train_texts, train_labels = zip(*train_data)\n","val_texts, val_labels = zip(*val_data)\n","test_texts, test_labels = zip(*test_data)\n","\n"]},{"cell_type":"markdown","id":"157a2726","metadata":{"id":"157a2726"},"source":["Creating the custom DataLoader."]},{"cell_type":"code","source":["# Define a custom dataset class for hate speech detection using PyTorch\n","class HateSpeechDataset(torch.utils.data.Dataset):\n","\n","    # Initialize the dataset object\n","    def __init__(self, texts, labels, tokenizer, max_len):\n","        # Store the list of textual samples\n","        self.texts = texts\n","        # Store the list of labels corresponding to each text sample\n","        self.labels = labels\n","        # Store the tokenizer instance which will convert text to tokens\n","        self.tokenizer = tokenizer\n","        # Store the maximum token length for sequences\n","        self.max_len = max_len\n","\n","    # Return the total number of samples in the dataset\n","    def __len__(self):\n","        return len(self.texts)\n","\n","    # Fetch and return a single data sample given its index\n","    def __getitem__(self, item):\n","        # Retrieve the text and its corresponding label using the provided index\n","        text = self.texts[item]\n","        label = self.labels[item]\n","\n","        # Tokenize the text using the provided tokenizer\n","        # This converts the text to a format suitable for model input\n","        encoding = self.tokenizer.encode_plus(\n","            text,\n","            add_special_tokens=True,   # Add special tokens like [CLS], [SEP]\n","            max_length=self.max_len,   # Ensure the sequence doesn't exceed the max length\n","            padding='max_length',      # Pad short sequences to the max length\n","            truncation=True,           # Truncate sequences exceeding the max length\n","            return_tensors='pt'        # Return data as PyTorch tensors\n","        )\n","\n","        # Return a dictionary containing the tokenized data and the label\n","        return {\n","            # The token IDs of the text\n","            'input_ids': encoding['input_ids'].flatten(),\n","            # A mask to indicate real tokens (1) vs padded tokens (0)\n","            'attention_mask': encoding['attention_mask'].flatten(),\n","            # The corresponding label of the text sample\n","            'labels': torch.tensor(label, dtype=torch.long)\n","        }\n"],"metadata":{"id":"K3RaLsxW1SK_"},"id":"K3RaLsxW1SK_","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"e99a29ae","metadata":{"id":"e99a29ae"},"source":["Defining the training arguments."]},{"cell_type":"code","execution_count":null,"id":"bc0efa5c","metadata":{"id":"bc0efa5c"},"outputs":[],"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model.to(device)\n","\n","from transformers import EarlyStoppingCallback\n","\n","#Define training arguments\n","training_args = TrainingArguments(\n","    output_dir='./results',\n","    num_train_epochs=3,\n","    per_device_train_batch_size=16,\n","    per_device_eval_batch_size=16,\n","    warmup_steps=500,\n","    weight_decay=0.01,\n","    logging_dir='./logs',\n","    logging_steps=500,\n","    evaluation_strategy='steps',\n","    eval_steps=500,\n","    load_best_model_at_end=True,  # Set load_best_model_at_end to True\n",")\n","\n","max_len = 128"]},{"cell_type":"markdown","id":"37b9c903","metadata":{"id":"37b9c903"},"source":["Defining the function to compute the accuracy and f1 scores."]},{"cell_type":"code","execution_count":null,"id":"d2871e4f","metadata":{"id":"d2871e4f"},"outputs":[],"source":["from sklearn.metrics import accuracy_score, f1_score\n","from transformers import EvalPrediction\n","import numpy as np\n","\n","# Define evaluation metrics function\n","def compute_metrics(p: EvalPrediction):\n","    preds = np.argmax(p.predictions, axis=1)\n","    return {\n","        'accuracy': accuracy_score(p.label_ids, preds),\n","        'f1': f1_score(p.label_ids, preds, average='weighted')\n","    }"]},{"cell_type":"markdown","id":"087f5f24","metadata":{"id":"087f5f24"},"source":["Importing required libraries and modules, and running the training and evaluation loops using the Huggingface Trainer class with k-fold cross validation."]},{"cell_type":"code","execution_count":null,"id":"33605a64","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1529},"executionInfo":{"elapsed":3543305,"status":"error","timestamp":1692369228689,"user":{"displayName":"Eszter Zsisku","userId":"05092016175184633832"},"user_tz":-60},"id":"33605a64","outputId":"12850f12-3d1f-4238-d413-ad126f0a03a3"},"outputs":[{"name":"stdout","output_type":"stream","text":["Training on Fold 1\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='2151' max='2151' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [2151/2151 14:49, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>0.460500</td>\n","      <td>0.431319</td>\n","      <td>0.846878</td>\n","      <td>0.846420</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.334700</td>\n","      <td>0.394549</td>\n","      <td>0.853157</td>\n","      <td>0.854008</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>0.276400</td>\n","      <td>0.378301</td>\n","      <td>0.877921</td>\n","      <td>0.877891</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>0.197700</td>\n","      <td>0.404660</td>\n","      <td>0.884548</td>\n","      <td>0.884364</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='180' max='180' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [180/180 00:20]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Fold 1 - Validation Accuracy: 0.8779, Validation F1: 0.8779\n","Training on Fold 2\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='1500' max='2151' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1500/2151 10:31 < 04:34, 2.37 it/s, Epoch 2/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>0.465300</td>\n","      <td>0.337902</td>\n","      <td>0.874084</td>\n","      <td>0.874394</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.326400</td>\n","      <td>0.342222</td>\n","      <td>0.879316</td>\n","      <td>0.879456</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>0.267200</td>\n","      <td>0.348776</td>\n","      <td>0.880712</td>\n","      <td>0.881493</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='180' max='180' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [180/180 00:20]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Fold 2 - Validation Accuracy: 0.8741, Validation F1: 0.8744\n","Training on Fold 3\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='1500' max='2151' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1500/2151 10:31 < 04:34, 2.37 it/s, Epoch 2/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>0.468400</td>\n","      <td>0.320498</td>\n","      <td>0.864318</td>\n","      <td>0.864716</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.331200</td>\n","      <td>0.335538</td>\n","      <td>0.870945</td>\n","      <td>0.871398</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>0.265500</td>\n","      <td>0.462243</td>\n","      <td>0.880712</td>\n","      <td>0.881160</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='180' max='180' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [180/180 00:20]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Fold 3 - Validation Accuracy: 0.8643, Validation F1: 0.8647\n","Training on Fold 4\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='1500' max='2151' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1500/2151 10:34 < 04:35, 2.36 it/s, Epoch 2/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>0.460400</td>\n","      <td>0.325701</td>\n","      <td>0.866062</td>\n","      <td>0.866373</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.338500</td>\n","      <td>0.328792</td>\n","      <td>0.871992</td>\n","      <td>0.872030</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>0.270800</td>\n","      <td>0.389101</td>\n","      <td>0.877572</td>\n","      <td>0.877520</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='180' max='180' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [180/180 00:20]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Fold 4 - Validation Accuracy: 0.8661, Validation F1: 0.8664\n","Training on Fold 5\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='1500' max='2151' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1500/2151 10:37 < 04:36, 2.35 it/s, Epoch 2/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>0.452400</td>\n","      <td>0.400665</td>\n","      <td>0.833915</td>\n","      <td>0.835107</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.318400</td>\n","      <td>0.474451</td>\n","      <td>0.856246</td>\n","      <td>0.854193</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>0.260700</td>\n","      <td>0.632115</td>\n","      <td>0.854501</td>\n","      <td>0.855506</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='180' max='180' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [180/180 00:20]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Fold 5 - Validation Accuracy: 0.8339, Validation F1: 0.8351\n"]},{"ename":"NameError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-31cbb650cabd>\u001b[0m in \u001b[0;36m<cell line: 56>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;31m# Step 6: Evaluate on Test Set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m \u001b[0meval_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Test Accuracy: {eval_result['eval_accuracy']:.4f}, Test F1: {eval_result['eval_f1']:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'test_dataset' is not defined"]}],"source":["from sklearn.model_selection import StratifiedKFold\n","\n","# Create a list to store the state dictionaries of the models for each fold\n","model_weights = []\n","\n","# Combine the train and validate sets (texts and labels)\n","train_val_texts = train_texts + val_texts\n","train_val_labels = train_labels + val_labels\n","\n","# Initialize StratifiedKFold\n","num_splits = 5\n","skf = StratifiedKFold(n_splits=num_splits, shuffle=True, random_state=42)\n","\n","# Create the k different training and validation splits\n","folds = []\n","for train_index, val_index in skf.split(train_val_texts, train_val_labels):\n","    train_texts_fold = [train_val_texts[i] for i in train_index]\n","    val_texts_fold = [train_val_texts[i] for i in val_index]\n","    train_labels_fold = [train_val_labels[i] for i in train_index]\n","    val_labels_fold = [train_val_labels[i] for i in val_index]\n","\n","    folds.append((train_texts_fold, train_labels_fold, val_texts_fold, val_labels_fold))\n","\n","# Iterate over the k folds\n","for fold_idx, (train_texts_fold, train_labels_fold, val_texts_fold, val_labels_fold) in enumerate(folds):\n","    print(f\"Training on Fold {fold_idx + 1}\")\n","\n","    # Create the training and validation datasets\n","    train_dataset_fold = HateSpeechDataset(train_texts_fold, train_labels_fold, tokenizer, max_len)\n","    val_dataset_fold = HateSpeechDataset(val_texts_fold, val_labels_fold, tokenizer, max_len)\n","\n","    # Reinitialize the model for each fold\n","    model = AutoModelForSequenceClassification.from_pretrained(model_name)\n","\n","    # Create the Trainer\n","    trainer = Trainer(\n","        model=model,\n","        args=training_args,\n","        train_dataset=train_dataset_fold,\n","        eval_dataset=val_dataset_fold,\n","        compute_metrics=compute_metrics,\n","        callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],\n","    )\n","\n","    # Train the model on the current fold\n","    trainer.train()\n","\n","    # Save the state dictionary of the model for the current fold\n","    model_weights.append(model.state_dict())\n","\n","    # Evaluate on the current fold\n","    eval_result = trainer.evaluate(val_dataset_fold)\n","    print(f\"Fold {fold_idx + 1} - Validation Accuracy: {eval_result['eval_accuracy']:.4f}, Validation F1: {eval_result['eval_f1']:.4f}\")\n","\n"]},{"cell_type":"markdown","id":"9d41a7c5","metadata":{"id":"9d41a7c5"},"source":["Processing data."]},{"cell_type":"code","execution_count":null,"id":"93c0a751","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":35020,"status":"ok","timestamp":1692370517658,"user":{"displayName":"Eszter Zsisku","userId":"05092016175184633832"},"user_tz":-60},"id":"93c0a751","outputId":"36a05829-8dad-4f94-c84b-0d6d1dd31bfd"},"outputs":[{"data":{"text/html":[],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Test Accuracy: 0.8346, Test F1: 0.8358\n"]}],"source":["# Step 6: Evaluate on Test Set\n","\n","test_dataset = HateSpeechDataset(test_texts, test_labels, tokenizer, max_len)\n","\n","eval_result = trainer.evaluate(test_dataset)\n","print(f\"Test Accuracy: {eval_result['eval_accuracy']:.4f}, Test F1: {eval_result['eval_f1']:.4f}\")\n","\n","# Get the predicted labels for the test dataset\n","test_predictions = trainer.predict(test_dataset).predictions\n","test_predicted_labels = np.argmax(test_predictions, axis=1)\n","\n","# Create a dictionary to store the collected information\n","test_data_dict = {\n","    \"original_sentence\": test_texts,  # Original sentences from the test dataset\n","    \"hate_label\": test_labels,  # Ground truth hate labels from the HateSpeechDataset\n","    \"predicted_label\": test_predicted_labels,  # Predicted labels from the model\n","}\n","\n","# Create a DataFrame from the dictionary\n","test_results_df = pd.DataFrame(test_data_dict)"]},{"cell_type":"markdown","id":"a2a64309","metadata":{"id":"a2a64309"},"source":["Averaging the weights from the cross validation process."]},{"cell_type":"code","execution_count":null,"id":"c47d39d4","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1521,"status":"ok","timestamp":1692370648708,"user":{"displayName":"Eszter Zsisku","userId":"05092016175184633832"},"user_tz":-60},"id":"c47d39d4","outputId":"41dd5844-7245-4e79-a471-b92aa63519c3"},"outputs":[{"data":{"text/plain":["<All keys matched successfully>"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["from collections import OrderedDict\n","\n","def average_model_weights(model_weights):\n","    # Initialize a dictionary to store the sum of the weights\n","    avg_weights = OrderedDict()\n","\n","    # Iterate through each state dictionary (weights for each fold)\n","    for state_dict in model_weights:\n","        for key, value in state_dict.items():\n","            # If the key is not in avg_weights, initialize it with zeros\n","            if key not in avg_weights:\n","                avg_weights[key] = torch.zeros_like(value)\n","\n","            # Add the value to the corresponding key in avg_weights\n","            avg_weights[key] += value\n","\n","    # Divide by the number of models to obtain the average\n","    for key in avg_weights:\n","        avg_weights[key] /= len(model_weights)\n","\n","    return avg_weights\n","\n","# Get the average weights\n","avg_weights = average_model_weights(model_weights)\n","\n","# Create a new model instance and load the averaged weights\n","ensemble_model = AutoModelForSequenceClassification.from_pretrained(model_name)\n","ensemble_model.load_state_dict(avg_weights)\n"]},{"cell_type":"markdown","id":"a0e123cd","metadata":{"id":"a0e123cd"},"source":["Saving the averaged weights model to the HuggingFace Hub."]},{"cell_type":"code","execution_count":null,"id":"36850ba7","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":158,"referenced_widgets":["a447492021eb43d3bcd5bab0c16ee97d","49287792735d4d089fdd0a868abf8207","d3b0284559bf48f59e04755b8b5bc38f","00676d6ac48f4291a08e445b67986ef4","e6df03e2020d474b84b2232384960b93","64985d85bfaa424a8a5c4e7686bc43b4","39bb13ea8c154d0eaf98d9dacf60ccf3","b36c0ca0d1724eb18f71b71857c3967d","03c884f2f56a428bbeb0cb16b3bf8722","8a36965dfbe84617a242afc4f2886867","310d1ba2ab5b45a8a0238d7bb398ba3f"]},"executionInfo":{"elapsed":19255,"status":"ok","timestamp":1692370675690,"user":{"displayName":"Eszter Zsisku","userId":"05092016175184633832"},"user_tz":-60},"id":"36850ba7","outputId":"2a66f8e4-b07d-459d-ce00-ccc82657cbfc"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a447492021eb43d3bcd5bab0c16ee97d","version_major":2,"version_minor":0},"text/plain":["pytorch_model.bin:   0%|          | 0.00/499M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["('EZiisk/EZ_finetune_Vidgen_model_RHS_ensemble_tokenizer/tokenizer_config.json',\n"," 'EZiisk/EZ_finetune_Vidgen_model_RHS_ensemble_tokenizer/special_tokens_map.json',\n"," 'EZiisk/EZ_finetune_Vidgen_model_RHS_ensemble_tokenizer/vocab.json',\n"," 'EZiisk/EZ_finetune_Vidgen_model_RHS_ensemble_tokenizer/merges.txt',\n"," 'EZiisk/EZ_finetune_Vidgen_model_RHS_ensemble_tokenizer/added_tokens.json',\n"," 'EZiisk/EZ_finetune_Vidgen_model_RHS_ensemble_tokenizer/tokenizer.json')"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["# Saving new ensemble model with averaged weights to HuggingFace Hub\n","\n","ensemble_model.save_pretrained(\"EZiisk/EZ_finetune_Vidgen_model_RHS_ensemble\", push_to_hub = True)\n","tokenizer.save_pretrained(\"EZiisk/EZ_finetune_Vidgen_model_RHS_ensemble_tokenizer\", push_to_hub = True)"]},{"cell_type":"markdown","id":"f3a5d238","metadata":{"id":"f3a5d238"},"source":["Producing the dataframe of the test sentences, their ground truth labels and the predicted labels from the model."]},{"cell_type":"code","execution_count":null,"id":"a52286ad","metadata":{"id":"a52286ad"},"outputs":[],"source":["# Inner merge based on the condition where 'sentences' matches 'original_sentence'\n","merged_df = data.merge(test_results_df, left_on='sentences', right_on='original_sentence', how='inner')\n","\n","# Drop the duplicate 'original_sentence' column after the merge\n","merged_df.drop('original_sentence', axis=1, inplace=True)"]},{"cell_type":"code","execution_count":null,"id":"1c905e95","metadata":{"id":"1c905e95"},"outputs":[],"source":["columns_to_drop = ['hate_label_y']\n","merged_df = merged_df.drop(columns_to_drop, axis=1)"]},{"cell_type":"code","execution_count":null,"id":"3099cafa","metadata":{"id":"3099cafa"},"outputs":[],"source":["column_name_mapping = {\n","    'hate_label_x': 'hate_label',\n","}\n","\n","# Rename the columns using the dictionary\n","merged_df.rename(columns=column_name_mapping, inplace=True)"]},{"cell_type":"markdown","id":"02032af9","metadata":{"id":"02032af9"},"source":["Processing data."]},{"cell_type":"code","execution_count":null,"id":"dbf8a184","metadata":{"id":"dbf8a184"},"outputs":[],"source":["columns_to_drop = ['clean_sentences']\n","merged_df = merged_df.drop(columns_to_drop, axis=1)"]},{"cell_type":"markdown","id":"c0a56596","metadata":{"id":"c0a56596"},"source":["Importing required libraries and modules."]},{"cell_type":"code","execution_count":null,"id":"e0e98b56","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"executionInfo":{"elapsed":19,"status":"ok","timestamp":1692370809088,"user":{"displayName":"Eszter Zsisku","userId":"05092016175184633832"},"user_tz":-60},"id":"e0e98b56","outputId":"59b52441-6eda-45b4-8373-2b7bbbdd66f6"},"outputs":[{"data":{"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{},"output_type":"display_data"},{"data":{"application/javascript":["download(\"download_c0d637ab-2cbb-4b47-a8e9-1d714ba3842b\", \"2.59_finetune_test_dataset_analysis\", 442122)"],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{},"output_type":"display_data"}],"source":["from google.colab import files\n","\n","merged_df.to_csv(\"2.59_finetune_test_dataset_analysis\", index=False)\n","files.download(\"2.59_finetune_test_dataset_analysis\")"]}],"metadata":{"colab":{"provenance":[]},"language_info":{"name":"python"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":5}