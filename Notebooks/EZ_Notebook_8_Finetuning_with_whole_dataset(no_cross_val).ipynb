{"cells":[{"cell_type":"markdown","id":"5579752e","metadata":{"id":"5579752e"},"source":["# This notebook fine-tunes the Vidgen et al. (2021) model using the entire fine-tuning dataset without cross-validation."]},{"cell_type":"markdown","id":"a4ab2a82","metadata":{"id":"a4ab2a82"},"source":["Importing required libraries and modules."]},{"cell_type":"code","execution_count":null,"id":"d0f55d8e","metadata":{"id":"d0f55d8e"},"outputs":[],"source":["import pandas as pd\n","from tqdm import tqdm\n","from sklearn.model_selection import train_test_split\n","import torch"]},{"cell_type":"code","execution_count":null,"id":"503fa77a","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4036,"status":"ok","timestamp":1692292994915,"user":{"displayName":"Eszter Zsisku","userId":"05092016175184633832"},"user_tz":-60},"id":"503fa77a","outputId":"1460525b-2d1c-4256-ae45-7ba70c2487cc"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.31.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.16.4)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.2)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n"]}],"source":["!pip install transformers"]},{"cell_type":"markdown","id":"21ccefba","metadata":{"id":"21ccefba"},"source":["Loading data."]},{"cell_type":"code","execution_count":null,"id":"2840b879","metadata":{"id":"2840b879"},"outputs":[],"source":["data = pd.read_csv('Notebook_8_9_10_fine_tune_final.csv')"]},{"cell_type":"markdown","id":"598d579f","metadata":{"id":"598d579f"},"source":["Importing required libraries and modules and model"]},{"cell_type":"code","execution_count":null,"id":"3fd10894","metadata":{"id":"3fd10894"},"outputs":[],"source":["from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\n","\n","# 1. Load the pre-trained model and tokenizer\n","model_name = \"facebook/roberta-hate-speech-dynabench-r4-target\"\n","model = AutoModelForSequenceClassification.from_pretrained(model_name)\n","tokenizer = AutoTokenizer.from_pretrained(model_name)"]},{"cell_type":"markdown","id":"67ee9f85","metadata":{"id":"67ee9f85"},"source":["Logging into Huggingface to allow for fine-tune model to be uploaded onto account."]},{"cell_type":"code","execution_count":null,"id":"d35738b5","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11277,"status":"ok","timestamp":1692293012620,"user":{"displayName":"Eszter Zsisku","userId":"05092016175184633832"},"user_tz":-60},"id":"d35738b5","outputId":"c819ead1-bda7-4db2-d1e8-23864f9f841e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.16.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.12.2)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2023.6.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.31.0)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.7.1)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (23.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.2.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2023.7.22)\n","\n","    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n","    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n","    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n","    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n","    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n","    \n","    A token is already saved on your machine. Run `huggingface-cli whoami` to get more information or `huggingface-cli logout` if you want to log out.\n","    Setting a new token will erase the existing one.\n","    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n","Token: \n","Add token as git credential? (Y/n) n\n","Token is valid (permission: write).\n","Your token has been saved to /root/.cache/huggingface/token\n","Login successful\n"]}],"source":["!pip install huggingface_hub\n","!huggingface-cli login"]},{"cell_type":"markdown","id":"5e72f643","metadata":{"id":"5e72f643"},"source":["Processing data."]},{"cell_type":"code","execution_count":null,"id":"b58af712","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1034,"status":"ok","timestamp":1692293013650,"user":{"displayName":"Eszter Zsisku","userId":"05092016175184633832"},"user_tz":-60},"id":"b58af712","outputId":"6f34d358-8dbf-4fb2-adb6-b7c09f4c14a2"},"outputs":[{"name":"stdout","output_type":"stream","text":["EZiisk\n"]}],"source":["!huggingface-cli whoami"]},{"cell_type":"markdown","id":"b78903b2","metadata":{"id":"b78903b2"},"source":["Importing required libraries and modules and preprocessing sentences."]},{"cell_type":"code","execution_count":null,"id":"17d637ee","metadata":{"id":"17d637ee"},"outputs":[],"source":["import re\n","import string\n","\n","def preprocess_sentence(sentence):\n","  # no lowercasing or punctuation removal as assumed to carry semantic information\n","    sentence = re.sub(r'\\\\n', ' ', sentence)\n","    sentence = re.sub(r'\\s+', ' ', sentence).strip()\n","    return sentence\n","\n","# Apply the preprocess_sentence function to the 'sentences' column\n","data['sentences'] = data['sentences'].apply(preprocess_sentence)\n"]},{"cell_type":"markdown","id":"6c2d9efc","metadata":{"id":"6c2d9efc"},"source":["Creating the train/validation/test split by stratifying the data using the gold label column."]},{"cell_type":"code","execution_count":null,"id":"4c06366d","metadata":{"id":"4c06366d"},"outputs":[],"source":["# Group data by \"gold_label\" and create lists of texts and labels for each group\n","grouped_data = data.groupby('gold_label')\n","grouped_texts = [group['sentences'].tolist() for _, group in grouped_data]\n","grouped_labels = [group['hate_label'].tolist() for _, group in grouped_data]\n","\n","# Initialize lists to store train, validation, and test data\n","train_texts, val_texts, test_texts = [], [], []\n","train_labels, val_labels, test_labels = [], [], []\n","\n","# Split each group into train, validation, and test sets\n","for texts, labels in zip(grouped_texts, grouped_labels):\n","    train_texts_group, test_texts_group, train_labels_group, test_labels_group = train_test_split(texts, labels, test_size=0.15, stratify=labels, random_state=42)\n","    train_texts_group, val_texts_group, train_labels_group, val_labels_group = train_test_split(train_texts_group, train_labels_group, test_size=0.1765, stratify=train_labels_group, random_state=42)\n","\n","    train_texts.extend(train_texts_group)\n","    val_texts.extend(val_texts_group)\n","    test_texts.extend(test_texts_group)\n","    train_labels.extend(train_labels_group)\n","    val_labels.extend(val_labels_group)\n","    test_labels.extend(test_labels_group)\n"]},{"cell_type":"markdown","id":"e16bad4e","metadata":{"id":"e16bad4e"},"source":["Creating the custom DataLoader."]},{"cell_type":"code","source":["# Define a custom dataset class for hate speech detection using PyTorch\n","class HateSpeechDataset(torch.utils.data.Dataset):\n","\n","    # Initialize the dataset object\n","    def __init__(self, texts, labels, tokenizer, max_len):\n","        # Store the list of textual samples\n","        self.texts = texts\n","        # Store the list of labels corresponding to each text sample\n","        self.labels = labels\n","        # Store the tokenizer instance which will convert text to tokens\n","        self.tokenizer = tokenizer\n","        # Store the maximum token length for sequences\n","        self.max_len = max_len\n","\n","    # Return the total number of samples in the dataset\n","    def __len__(self):\n","        return len(self.texts)\n","\n","    # Fetch and return a single data sample given its index\n","    def __getitem__(self, item):\n","        # Retrieve the text and its corresponding label using the provided index\n","        text = self.texts[item]\n","        label = self.labels[item]\n","\n","        # Tokenize the text using the provided tokenizer\n","        # This converts the text to a format suitable for model input\n","        encoding = self.tokenizer.encode_plus(\n","            text,\n","            add_special_tokens=True,   # Add special tokens like [CLS], [SEP]\n","            max_length=self.max_len,   # Ensure the sequence doesn't exceed the max length\n","            padding='max_length',      # Pad short sequences to the max length\n","            truncation=True,           # Truncate sequences exceeding the max length\n","            return_tensors='pt'        # Return data as PyTorch tensors\n","        )\n","\n","        # Return a dictionary containing the tokenized data and the label\n","        return {\n","            # The token IDs of the text\n","            'input_ids': encoding['input_ids'].flatten(),\n","            # A mask to indicate real tokens (1) vs padded tokens (0)\n","            'attention_mask': encoding['attention_mask'].flatten(),\n","            # The corresponding label of the text sample\n","            'labels': torch.tensor(label, dtype=torch.long)\n","        }\n"],"metadata":{"id":"P2ZO0jGUy1cG"},"id":"P2ZO0jGUy1cG","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"6d1753ff","metadata":{"id":"6d1753ff"},"source":["Creating the train/validation and test datasets"]},{"cell_type":"code","execution_count":null,"id":"0fd46078","metadata":{"id":"0fd46078"},"outputs":[],"source":["# Step 4: Create Datasets\n","max_len = 128\n","train_dataset = HateSpeechDataset(train_texts, train_labels, tokenizer, max_len)\n","val_dataset = HateSpeechDataset(val_texts, val_labels, tokenizer, max_len)\n","test_dataset = HateSpeechDataset(test_texts, test_labels, tokenizer, max_len)"]},{"cell_type":"code","execution_count":null,"id":"99352589","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6068,"status":"ok","timestamp":1692293020260,"user":{"displayName":"Eszter Zsisku","userId":"05092016175184633832"},"user_tz":-60},"id":"99352589","outputId":"dcc32842-c499-4ffd-8d06-299bd09f8112"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.21.0)\n","Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.31.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.1)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n","Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.0.1+cu118)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.12.2)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.16.4)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.31.0)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.13.3)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.3.2)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.66.1)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers[torch]) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers[torch]) (4.7.1)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (3.27.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (16.0.6)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.2.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2023.7.22)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"]}],"source":["!pip install accelerate -U transformers[torch]"]},{"cell_type":"markdown","id":"b5e442dd","metadata":{"id":"b5e442dd"},"source":["Importing required libraries and modules, and running the training and evaluation loops using the Hugginface Trainer class."]},{"cell_type":"code","execution_count":null,"id":"c9c9b92e","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":295},"id":"c9c9b92e","outputId":"e81a8ddf-1e72-431e-c903-ebfc8da04033"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='2000' max='2214' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [2000/2214 13:13 < 01:25, 2.52 it/s, Epoch 2/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>0.454300</td>\n","      <td>0.402625</td>\n","      <td>0.849250</td>\n","      <td>0.845649</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.335900</td>\n","      <td>0.318017</td>\n","      <td>0.855959</td>\n","      <td>0.856966</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>0.278400</td>\n","      <td>0.333650</td>\n","      <td>0.883189</td>\n","      <td>0.883659</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>0.189400</td>\n","      <td>0.387020</td>\n","      <td>0.887135</td>\n","      <td>0.886834</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='266' max='159' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [159/159 00:29]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Test Accuracy: 0.8469, Test F1: 0.8480\n"]}],"source":["from sklearn.metrics import accuracy_score, f1_score\n","from transformers import EvalPrediction# Define evaluation metrics function\n","import numpy as np\n","\n","def compute_metrics(p: EvalPrediction):\n","    preds = np.argmax(p.predictions, axis=1)\n","    return {\n","        'accuracy': accuracy_score(p.label_ids, preds),\n","        'f1': f1_score(p.label_ids, preds, average='weighted')\n","    }\n","\n","# Step 5: Fine-tuning the Model using the Trainer class\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model.to(device)\n","\n","from transformers import EarlyStoppingCallback\n","\n","training_args = TrainingArguments(\n","    output_dir='./results',\n","    num_train_epochs=3,\n","    per_device_train_batch_size=16,\n","    per_device_eval_batch_size=16,\n","    warmup_steps=500,\n","    weight_decay=0.01,\n","    logging_dir='./logs',\n","    logging_steps=500,\n","    evaluation_strategy='steps',\n","    eval_steps=500,\n","    load_best_model_at_end=True,  # Set load_best_model_at_end to True\n","    # Remove 'early_stopping_patience' from here\n",")\n","\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_dataset,\n","    eval_dataset=val_dataset,\n","    compute_metrics=compute_metrics,\n","    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],  # Use EarlyStoppingCallback here\n",")\n","\n","# Train the model\n","trainer.train()\n","\n","# Step 6: Evaluate on Test Set\n","eval_result = trainer.evaluate(test_dataset)\n","print(f\"Test Accuracy: {eval_result['eval_accuracy']:.4f}, Test F1: {eval_result['eval_f1']:.4f}\")\n","\n","# Get the predicted labels for the test dataset\n","test_predictions = trainer.predict(test_dataset).predictions\n","test_predicted_labels = np.argmax(test_predictions, axis=1)\n","\n","# Create a dictionary to store the collected information\n","test_data_dict = {\n","    \"original_sentence\": test_texts,  # Original sentences from the test dataset\n","    \"hate_label\": test_labels,  # Ground truth hate labels from the HateSpeechDataset\n","    \"predicted_label\": test_predicted_labels,  # Predicted labels from the model\n","}\n","\n","# Create a DataFrame from the dictionary\n","test_results_df = pd.DataFrame(test_data_dict)"]},{"cell_type":"markdown","id":"0d412646","metadata":{"id":"0d412646"},"source":["Producing the dataframe of the test sentences, their ground truth labels and the predicted labels from the model."]},{"cell_type":"code","execution_count":null,"id":"8aadbbba","metadata":{"id":"8aadbbba"},"outputs":[],"source":["# Inner merge based on the condition where 'sentences' matches 'original_sentence'\n","merged_df = data.merge(test_results_df, left_on='sentences', right_on='original_sentence', how='inner')\n","\n","# Drop the duplicate 'original_sentence' column after the merge\n","merged_df.drop('original_sentence', axis=1, inplace=True)"]},{"cell_type":"code","execution_count":null,"id":"bf985bc4","metadata":{"id":"bf985bc4"},"outputs":[],"source":["columns_to_drop = ['hate_label_y']\n","merged_df = merged_df.drop(columns_to_drop, axis=1)"]},{"cell_type":"code","execution_count":null,"id":"2b644a6a","metadata":{"id":"2b644a6a"},"outputs":[],"source":["column_name_mapping = {\n","    'hate_label_x': 'hate_label',\n","}\n","\n","# Rename the columns using the dictionary\n","merged_df.rename(columns=column_name_mapping, inplace=True)"]},{"cell_type":"code","execution_count":null,"id":"3b1b93a7","metadata":{"id":"3b1b93a7"},"outputs":[],"source":["columns_to_drop = ['clean_sentences']\n","merged_df = merged_df.drop(columns_to_drop, axis=1)"]},{"cell_type":"markdown","id":"bacb1ce1","metadata":{"id":"bacb1ce1"},"source":["Downloading the CSV file"]},{"cell_type":"code","execution_count":null,"id":"bf59333d","metadata":{"id":"bf59333d"},"outputs":[],"source":["from google.colab import files\n","\n","merged_df.to_csv(\"2.55_finetune_test_dataset_analysis\", index=False)\n","files.download(\"2.55_finetune_test_dataset_analysis\")"]},{"cell_type":"markdown","id":"6d702207","metadata":{"id":"6d702207"},"source":["Saving the model to the Huggingface Hub"]},{"cell_type":"code","execution_count":null,"id":"622f607b","metadata":{"id":"622f607b"},"outputs":[],"source":["# model_path = \"EZiisk/EZ_finetune_Vidgen_model_RHS\"\n","# save_path = \"EZiisk/EZ_finetune_Vidgen_model_RHS_tokenizer\"\n","# model.save_pretrained(model_path, push_to_hub = True)\n","# tokenizer.save_pretrained(save_path, push_to_hub = True)"]}],"metadata":{"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}