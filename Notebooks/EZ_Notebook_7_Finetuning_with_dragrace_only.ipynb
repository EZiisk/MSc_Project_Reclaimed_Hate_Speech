{"cells":[{"cell_type":"markdown","id":"0cb8d70b","metadata":{"id":"0cb8d70b"},"source":["# The notebook is dedicated to fine-tuning models using only the 'Drag Race' transcript data."]},{"cell_type":"markdown","id":"7c167888","metadata":{"id":"7c167888"},"source":["Importing required libraries and modules."]},{"cell_type":"code","execution_count":1,"id":"1df4654e","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10808,"status":"ok","timestamp":1692749407550,"user":{"displayName":"Eszter Zsisku","userId":"05092016175184633832"},"user_tz":-60},"id":"1df4654e","outputId":"f9fd17ee-854a-4adc-cb24-02974645ae4e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.32.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.16.4)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.2)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.7.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n"]}],"source":["!pip install transformers"]},{"cell_type":"code","execution_count":2,"id":"445b23e3","metadata":{"id":"445b23e3","executionInfo":{"status":"ok","timestamp":1692749410859,"user_tz":-60,"elapsed":3344,"user":{"displayName":"Eszter Zsisku","userId":"05092016175184633832"}}},"outputs":[],"source":["import pandas as pd\n","from tqdm import tqdm\n","from sklearn.model_selection import train_test_split\n","import torch"]},{"cell_type":"code","execution_count":3,"id":"d27b657c","metadata":{"id":"d27b657c","executionInfo":{"status":"ok","timestamp":1692749410860,"user_tz":-60,"elapsed":8,"user":{"displayName":"Eszter Zsisku","userId":"05092016175184633832"}}},"outputs":[],"source":["data = pd.read_csv('Notebook_6_7_dragrace_transcript_wrongpreds.csv')"]},{"cell_type":"markdown","id":"5dc0825d","metadata":{"id":"5dc0825d"},"source":["Adding ground truth hate_label column to drag race transcript data"]},{"cell_type":"code","execution_count":4,"id":"1bfd0fc0","metadata":{"id":"1bfd0fc0","executionInfo":{"status":"ok","timestamp":1692749410860,"user_tz":-60,"elapsed":7,"user":{"displayName":"Eszter Zsisku","userId":"05092016175184633832"}}},"outputs":[],"source":["data['hate_label'] = 0"]},{"cell_type":"markdown","id":"136bf689","metadata":{"id":"136bf689"},"source":["Importing required libraries and modules."]},{"cell_type":"code","execution_count":5,"id":"3db9d7a3","metadata":{"id":"3db9d7a3","executionInfo":{"status":"ok","timestamp":1692749422939,"user_tz":-60,"elapsed":12085,"user":{"displayName":"Eszter Zsisku","userId":"05092016175184633832"}}},"outputs":[],"source":["from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\n","\n","# 1. Load the pre-trained model and tokenizer\n","model_name = \"facebook/roberta-hate-speech-dynabench-r4-target\"\n","model = AutoModelForSequenceClassification.from_pretrained(model_name)\n","tokenizer = AutoTokenizer.from_pretrained(model_name)"]},{"cell_type":"markdown","id":"93f9341a","metadata":{"id":"93f9341a"},"source":["Preprocessing the dataset"]},{"cell_type":"code","execution_count":6,"id":"3208a8d8","metadata":{"id":"3208a8d8","executionInfo":{"status":"ok","timestamp":1692749422940,"user_tz":-60,"elapsed":33,"user":{"displayName":"Eszter Zsisku","userId":"05092016175184633832"}}},"outputs":[],"source":["import re\n","import string\n","\n","def preprocess_sentence(sentence):\n","  # no lowercasing or punctuation removal as assumed to carry semantic information\n","    sentence = re.sub(r'\\\\n', ' ', sentence)\n","    sentence = re.sub(r'\\s+', ' ', sentence).strip()\n","    return sentence\n","\n","# Apply the preprocess_sentence function to the 'sentences' column\n","data['sentences'] = data['sentences'].apply(preprocess_sentence)\n"]},{"cell_type":"markdown","id":"817a3b20","metadata":{"id":"817a3b20"},"source":["Preparing the texts and labels for classification."]},{"cell_type":"code","execution_count":7,"id":"2542aa02","metadata":{"id":"2542aa02","executionInfo":{"status":"ok","timestamp":1692749422941,"user_tz":-60,"elapsed":31,"user":{"displayName":"Eszter Zsisku","userId":"05092016175184633832"}}},"outputs":[],"source":["train_texts = data['sentences'].to_list()\n","train_labels = data['hate_label'].to_list()"]},{"cell_type":"markdown","id":"cec97f12","metadata":{"id":"cec97f12"},"source":["Loading the dataset from Vidgen et al. (2021) from which the test dataset will be taken for evaluating the fine-tuned model's performance."]},{"cell_type":"code","execution_count":8,"id":"48b18155","metadata":{"id":"48b18155","executionInfo":{"status":"ok","timestamp":1692749423285,"user_tz":-60,"elapsed":374,"user":{"displayName":"Eszter Zsisku","userId":"05092016175184633832"}}},"outputs":[],"source":["dataset2 = pd.read_csv('Notebook_7_Dynamically Generated Hate Dataset v0.2.3.csv')"]},{"cell_type":"code","execution_count":9,"id":"6d13258c","metadata":{"id":"6d13258c","executionInfo":{"status":"ok","timestamp":1692749423286,"user_tz":-60,"elapsed":6,"user":{"displayName":"Eszter Zsisku","userId":"05092016175184633832"}}},"outputs":[],"source":["from sklearn.preprocessing import LabelEncoder\n","encoder = LabelEncoder()"]},{"cell_type":"code","execution_count":10,"id":"6b672a57","metadata":{"id":"6b672a57","executionInfo":{"status":"ok","timestamp":1692749423649,"user_tz":-60,"elapsed":368,"user":{"displayName":"Eszter Zsisku","userId":"05092016175184633832"}}},"outputs":[],"source":["# Encode the original labels\n","encoded_labels = encoder.fit_transform(dataset2['label'])\n","\n","# Create a mapping dictionary to reverse the labels\n","mapping = {'nothate': 0, 'hate': 1}\n","\n","# Apply the mapping to reverse the labels\n","dataset2['Numeric_label'] = [mapping[label] for label in dataset2['label']]"]},{"cell_type":"markdown","id":"108244f3","metadata":{"id":"108244f3"},"source":["Creating variables to store the test dataset texts and labels from Vidgen et al. (2021)"]},{"cell_type":"code","execution_count":11,"id":"cd3567e4","metadata":{"id":"cd3567e4","executionInfo":{"status":"ok","timestamp":1692749423650,"user_tz":-60,"elapsed":13,"user":{"displayName":"Eszter Zsisku","userId":"05092016175184633832"}}},"outputs":[],"source":["dataset_test = dataset2.loc[dataset2['split'] == 'test']\n","test_texts = dataset_test['text'][dataset_test['round.base'] == 4].values.tolist()\n","test_labels = dataset_test['Numeric_label'][dataset_test['round.base'] == 4].values.tolist()"]},{"cell_type":"markdown","id":"0d27f7a9","metadata":{"id":"0d27f7a9"},"source":["Creating a custom Dataloader which will be used to process the train and test datasets in the pretrained RoBERTa model."]},{"cell_type":"code","source":["# Define a custom dataset class for hate speech detection using PyTorch\n","class HateSpeechDataset(torch.utils.data.Dataset):\n","\n","    # Initialize the dataset object\n","    def __init__(self, texts, labels, tokenizer, max_len):\n","        # Store the list of textual samples\n","        self.texts = texts\n","        # Store the list of labels corresponding to each text sample\n","        self.labels = labels\n","        # Store the tokenizer instance which will convert text to tokens\n","        self.tokenizer = tokenizer\n","        # Store the maximum token length for sequences\n","        self.max_len = max_len\n","\n","    # Return the total number of samples in the dataset\n","    def __len__(self):\n","        return len(self.texts)\n","\n","    # Fetch and return a single data sample given its index\n","    def __getitem__(self, item):\n","        # Retrieve the text and its corresponding label using the provided index\n","        text = self.texts[item]\n","        label = self.labels[item]\n","\n","        # Tokenize the text using the provided tokenizer\n","        # This converts the text to a format suitable for model input\n","        encoding = self.tokenizer.encode_plus(\n","            text,\n","            add_special_tokens=True,   # Add special tokens like [CLS], [SEP]\n","            max_length=self.max_len,   # Ensure the sequence doesn't exceed the max length\n","            padding='max_length',      # Pad short sequences to the max length\n","            truncation=True,           # Truncate sequences exceeding the max length\n","            return_tensors='pt'        # Return data as PyTorch tensors\n","        )\n","\n","        # Return a dictionary containing the tokenized data and the label\n","        return {\n","            # The token IDs of the text\n","            'input_ids': encoding['input_ids'].flatten(),\n","            # A mask to indicate real tokens (1) vs padded tokens (0)\n","            'attention_mask': encoding['attention_mask'].flatten(),\n","            # The corresponding label of the text sample\n","            'labels': torch.tensor(label, dtype=torch.long)\n","        }\n"],"metadata":{"id":"se4g1Xs9yEqd","executionInfo":{"status":"ok","timestamp":1692749423652,"user_tz":-60,"elapsed":14,"user":{"displayName":"Eszter Zsisku","userId":"05092016175184633832"}}},"id":"se4g1Xs9yEqd","execution_count":12,"outputs":[]},{"cell_type":"markdown","id":"06a47387","metadata":{"id":"06a47387"},"source":["Creating the train and test datasets using the custom dataloader function and defining the maximum length of texts to be tokenized."]},{"cell_type":"code","execution_count":13,"id":"d83f0be4","metadata":{"id":"d83f0be4","executionInfo":{"status":"ok","timestamp":1692749423653,"user_tz":-60,"elapsed":13,"user":{"displayName":"Eszter Zsisku","userId":"05092016175184633832"}}},"outputs":[],"source":["max_len = 128\n","train_dataset = HateSpeechDataset(train_texts, train_labels, tokenizer, max_len)\n","test_dataset = HateSpeechDataset(test_texts, test_labels, tokenizer, max_len)"]},{"cell_type":"code","execution_count":14,"id":"67f4f1a3","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13047,"status":"ok","timestamp":1692749436688,"user":{"displayName":"Eszter Zsisku","userId":"05092016175184633832"},"user_tz":-60},"id":"67f4f1a3","outputId":"ecbdced9-5f3e-4328-9086-d58c9c11bac7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.21.0)\n","Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.32.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.1)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n","Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.0.1+cu118)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.12.2)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.16.4)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.31.0)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.13.3)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.3.2)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.66.1)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers[torch]) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers[torch]) (4.7.1)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (3.27.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (16.0.6)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.2.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2023.7.22)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"]}],"source":["!pip install accelerate -U transformers[torch]"]},{"cell_type":"markdown","id":"270bf738","metadata":{"id":"270bf738"},"source":["The training loop for the fine-tuning process using the Huggingface Trainer calss. and the evaluation of the fine-tuned model using the test dataset."]},{"cell_type":"code","execution_count":15,"id":"cabe62cf","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":113},"id":"cabe62cf","outputId":"dbd9f192-d3cf-43ca-eaf7-73b127ad3d40","executionInfo":{"status":"ok","timestamp":1692753691605,"user_tz":-60,"elapsed":4254957,"user":{"displayName":"Eszter Zsisku","userId":"05092016175184633832"}}},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='195' max='195' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [195/195 1:03:48, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='64' max='64' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [64/64 06:20]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Test Accuracy: 0.5557, Test F1: 0.4286\n"]}],"source":["from sklearn.metrics import accuracy_score, f1_score\n","from transformers import EvalPrediction\n","import numpy as np\n","\n","# Define evaluation metrics function to include the accuracy and F1 scores using the prediction classes from the model\n","def compute_metrics(p: EvalPrediction):\n","    preds = np.argmax(p.predictions, axis=1)\n","    return {\n","        'accuracy': accuracy_score(p.label_ids, preds),\n","        'f1': f1_score(p.label_ids, preds, average='weighted')\n","    }\n","\n","# Step 5: Fine-tuning the Model using the Trainer class\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model.to(device)\n","\n","from transformers import EarlyStoppingCallback\n","\n","training_args = TrainingArguments(\n","    output_dir='./results',\n","    num_train_epochs=3,\n","    per_device_train_batch_size=16,\n","    per_device_eval_batch_size=16,\n","    warmup_steps=500,\n","    weight_decay=0.01,\n","    logging_dir='./logs',\n","    logging_steps=500,\n","    evaluation_strategy='steps',\n","    eval_steps=500,\n","    load_best_model_at_end=True,  # Set load_best_model_at_end to True\n","    # Remove 'early_stopping_patience' from here\n",")\n","\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_dataset,\n","    compute_metrics=compute_metrics,\n",")\n","\n","# Train the model\n","trainer.train()\n","\n","# Step 6: Evaluate on Test Set\n","eval_result = trainer.evaluate(test_dataset)\n","print(f\"Test Accuracy: {eval_result['eval_accuracy']:.4f}, Test F1: {eval_result['eval_f1']:.4f}\")\n"]}],"metadata":{"colab":{"provenance":[]},"language_info":{"name":"python"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":5}