{"cells":[{"cell_type":"markdown","id":"f2e028df","metadata":{"id":"f2e028df"},"source":["# The notebook fine-tunes the Vidgen et al. (2021) model using cross-validation, selecting the best fold."]},{"cell_type":"markdown","id":"e25c3e27","metadata":{"id":"e25c3e27"},"source":["Importing required libraries and modules."]},{"cell_type":"code","execution_count":null,"id":"56735eaf","metadata":{"id":"56735eaf"},"outputs":[],"source":["import pandas as pd\n","from tqdm import tqdm\n","from sklearn.model_selection import train_test_split\n","import torch"]},{"cell_type":"markdown","id":"d29dc7da","metadata":{"id":"d29dc7da"},"source":["Loading data."]},{"cell_type":"code","execution_count":null,"id":"756d7e8d","metadata":{"id":"756d7e8d"},"outputs":[],"source":["data = pd.read_csv('Notebook_8_9_10_fine_tune_final.csv')"]},{"cell_type":"code","execution_count":null,"id":"ed65bd5f","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4614,"status":"ok","timestamp":1692295863828,"user":{"displayName":"Eszter Zsisku","userId":"05092016175184633832"},"user_tz":-60},"id":"ed65bd5f","outputId":"c1ecbcd5-a5b6-493e-dc7b-0f2802f04fb2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.31.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.16.4)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.2)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n"]}],"source":["!pip install transformers"]},{"cell_type":"markdown","id":"26e9e94c","metadata":{"id":"26e9e94c"},"source":["Importing required libraries and modules and Vidgen et al's (2021) model."]},{"cell_type":"code","execution_count":null,"id":"4f620a28","metadata":{"id":"4f620a28"},"outputs":[],"source":["from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\n","\n","# 1. Load the pre-trained model and tokenizer\n","model_name = \"facebook/roberta-hate-speech-dynabench-r4-target\"\n","model = AutoModelForSequenceClassification.from_pretrained(model_name)\n","tokenizer = AutoTokenizer.from_pretrained(model_name)"]},{"cell_type":"markdown","id":"01fb4cff","metadata":{"id":"01fb4cff"},"source":["Logging into Huggingface to allow for fine-tune model to be uploaded onto account."]},{"cell_type":"code","execution_count":null,"id":"b1d4a264","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11007,"status":"ok","timestamp":1692295880684,"user":{"displayName":"Eszter Zsisku","userId":"05092016175184633832"},"user_tz":-60},"id":"b1d4a264","outputId":"66c298e7-1e94-4682-9eb7-21a0a9007edc"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.16.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.12.2)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2023.6.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.31.0)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.7.1)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (23.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.2.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2023.7.22)\n","\n","    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n","    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n","    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n","    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n","    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n","    \n","    A token is already saved on your machine. Run `huggingface-cli whoami` to get more information or `huggingface-cli logout` if you want to log out.\n","    Setting a new token will erase the existing one.\n","    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n","Token: \n","Add token as git credential? (Y/n) n\n","Token is valid (permission: write).\n","Your token has been saved to /root/.cache/huggingface/token\n","Login successful\n"]}],"source":["!pip install huggingface_hub\n","!huggingface-cli login"]},{"cell_type":"code","execution_count":null,"id":"b9b0c117","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":481,"status":"ok","timestamp":1692295881133,"user":{"displayName":"Eszter Zsisku","userId":"05092016175184633832"},"user_tz":-60},"id":"b9b0c117","outputId":"558de560-6072-40b5-ddfb-f7d1d13d6233"},"outputs":[{"name":"stdout","output_type":"stream","text":["EZiisk\n"]}],"source":["!huggingface-cli whoami"]},{"cell_type":"code","execution_count":null,"id":"4c37573d","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5906,"status":"ok","timestamp":1692295887028,"user":{"displayName":"Eszter Zsisku","userId":"05092016175184633832"},"user_tz":-60},"id":"4c37573d","outputId":"b7861f74-cbae-4cd0-d3a5-47fcb1833e4a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.21.0)\n","Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.31.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.1)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n","Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.0.1+cu118)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.12.2)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.16.4)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.31.0)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.13.3)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.3.2)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.66.1)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers[torch]) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers[torch]) (4.7.1)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (3.27.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (16.0.6)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.2.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2023.7.22)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"]}],"source":["!pip install accelerate -U transformers[torch]"]},{"cell_type":"markdown","id":"02bdadf0","metadata":{"id":"02bdadf0"},"source":["Importing required libraries and modules and preprocessing sentences."]},{"cell_type":"code","execution_count":null,"id":"cfcee03f","metadata":{"id":"cfcee03f"},"outputs":[],"source":["import re\n","import string\n","\n","def preprocess_sentence(sentence):\n","  # no lowercasing or punctuation removal as assumed to carry semantic information\n","    sentence = re.sub(r'\\\\n', ' ', sentence)\n","    sentence = re.sub(r'\\s+', ' ', sentence).strip()\n","    return sentence\n","\n","# Apply the preprocess_sentence function to the 'sentences' column\n","data['sentences'] = data['sentences'].apply(preprocess_sentence)\n"]},{"cell_type":"markdown","id":"0bb3000d","metadata":{"id":"0bb3000d"},"source":["Creating the train/validation/test split by stratifying the data using the gold label column."]},{"cell_type":"code","execution_count":null,"id":"ac9534c9","metadata":{"id":"ac9534c9"},"outputs":[],"source":["# Initialize lists to store train, validate, and test data\n","train_data, val_data, test_data = [], [], []\n","\n","# Group data by \"gold_label\" and create lists of texts and labels for each group\n","grouped_data = data.groupby('gold_label')\n","grouped_texts = [group['sentences'].tolist() for _, group in grouped_data]\n","grouped_labels = [group['hate_label'].tolist() for _, group in grouped_data]\n","\n","# Split each group into train (70%), validate (15%), and test (15%) sets\n","for texts, labels in zip(grouped_texts, grouped_labels):\n","    # Split into train (85%) and test (15%)\n","    train_texts_group, test_texts_group, train_labels_group, test_labels_group = train_test_split(\n","        texts, labels, test_size=0.15, stratify=labels, random_state=42\n","    )\n","\n","    # Split train set into train (82.35%) and validate (17.65%) to achieve a 70-15-15 split\n","    train_texts_group, val_texts_group, train_labels_group, val_labels_group = train_test_split(\n","        train_texts_group, train_labels_group, test_size=0.1765, stratify=train_labels_group, random_state=42\n","    )\n","\n","    train_data.extend(list(zip(train_texts_group, train_labels_group)))\n","    val_data.extend(list(zip(val_texts_group, val_labels_group)))\n","    test_data.extend(list(zip(test_texts_group, test_labels_group)))\n","\n","# Separate the train, validate, and test texts and labels\n","train_texts, train_labels = zip(*train_data)\n","val_texts, val_labels = zip(*val_data)\n","test_texts, test_labels = zip(*test_data)\n","\n"]},{"cell_type":"markdown","id":"74b1252a","metadata":{"id":"74b1252a"},"source":["Creating the custom DataLoader."]},{"cell_type":"code","source":["# Define a custom dataset class for hate speech detection using PyTorch\n","class HateSpeechDataset(torch.utils.data.Dataset):\n","\n","    # Initialize the dataset object\n","    def __init__(self, texts, labels, tokenizer, max_len):\n","        # Store the list of textual samples\n","        self.texts = texts\n","        # Store the list of labels corresponding to each text sample\n","        self.labels = labels\n","        # Store the tokenizer instance which will convert text to tokens\n","        self.tokenizer = tokenizer\n","        # Store the maximum token length for sequences\n","        self.max_len = max_len\n","\n","    # Return the total number of samples in the dataset\n","    def __len__(self):\n","        return len(self.texts)\n","\n","    # Fetch and return a single data sample given its index\n","    def __getitem__(self, item):\n","        # Retrieve the text and its corresponding label using the provided index\n","        text = self.texts[item]\n","        label = self.labels[item]\n","\n","        # Tokenize the text using the provided tokenizer\n","        # This converts the text to a format suitable for model input\n","        encoding = self.tokenizer.encode_plus(\n","            text,\n","            add_special_tokens=True,   # Add special tokens like [CLS], [SEP]\n","            max_length=self.max_len,   # Ensure the sequence doesn't exceed the max length\n","            padding='max_length',      # Pad short sequences to the max length\n","            truncation=True,           # Truncate sequences exceeding the max length\n","            return_tensors='pt'        # Return data as PyTorch tensors\n","        )\n","\n","        # Return a dictionary containing the tokenized data and the label\n","        return {\n","            # The token IDs of the text\n","            'input_ids': encoding['input_ids'].flatten(),\n","            # A mask to indicate real tokens (1) vs padded tokens (0)\n","            'attention_mask': encoding['attention_mask'].flatten(),\n","            # The corresponding label of the text sample\n","            'labels': torch.tensor(label, dtype=torch.long)\n","        }\n"],"metadata":{"id":"8Tw169_-z2wv"},"id":"8Tw169_-z2wv","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"26b5526d","metadata":{"id":"26b5526d"},"source":["Defining the training arguments."]},{"cell_type":"code","execution_count":null,"id":"985563ce","metadata":{"id":"985563ce"},"outputs":[],"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model.to(device)\n","\n","from transformers import EarlyStoppingCallback\n","\n","#Define training arguments\n","training_args = TrainingArguments(\n","    output_dir='./results',\n","    num_train_epochs=3,\n","    per_device_train_batch_size=16,\n","    per_device_eval_batch_size=16,\n","    warmup_steps=500,\n","    weight_decay=0.01,\n","    logging_dir='./logs',\n","    logging_steps=500,\n","    evaluation_strategy='steps',\n","    eval_steps=500,\n","    load_best_model_at_end=True,  # Set load_best_model_at_end to True\n",")\n","\n","max_len = 128"]},{"cell_type":"markdown","id":"1483616a","metadata":{"id":"1483616a"},"source":["Defining the function to compute the accuracy and f1 scores."]},{"cell_type":"code","execution_count":null,"id":"2409a88c","metadata":{"id":"2409a88c"},"outputs":[],"source":["from sklearn.metrics import accuracy_score, f1_score\n","from transformers import EvalPrediction\n","import numpy as np\n","\n","# Define evaluation metrics function\n","def compute_metrics(p: EvalPrediction):\n","    preds = np.argmax(p.predictions, axis=1)\n","    return {\n","        'accuracy': accuracy_score(p.label_ids, preds),\n","        'f1': f1_score(p.label_ids, preds, average='weighted')\n","    }"]},{"cell_type":"markdown","id":"bdf7516c","metadata":{"id":"bdf7516c"},"source":["Importing required libraries and modules, and running the training and evaluation loops using the Huggingface Trainer class with k-fold cross validation."]},{"cell_type":"code","execution_count":null,"id":"bfa7399f","metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":212},"id":"bfa7399f","outputId":"dd98f63e-aaeb-4d20-df4c-2fd6c67061c9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Training on Fold 1\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='2151' max='2151' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [2151/2151 14:51, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>0.460500</td>\n","      <td>0.431319</td>\n","      <td>0.846878</td>\n","      <td>0.846420</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.334700</td>\n","      <td>0.394549</td>\n","      <td>0.853157</td>\n","      <td>0.854008</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>0.276400</td>\n","      <td>0.378301</td>\n","      <td>0.877921</td>\n","      <td>0.877891</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>0.197700</td>\n","      <td>0.404660</td>\n","      <td>0.884548</td>\n","      <td>0.884364</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='180' max='180' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [180/180 00:20]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Fold 1 - Validation Accuracy: 0.8779, Validation F1: 0.8779\n","Training on Fold 2\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='1500' max='2151' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1500/2151 10:31 < 04:34, 2.37 it/s, Epoch 2/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>0.465300</td>\n","      <td>0.337902</td>\n","      <td>0.874084</td>\n","      <td>0.874394</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.326400</td>\n","      <td>0.342222</td>\n","      <td>0.879316</td>\n","      <td>0.879456</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>0.267200</td>\n","      <td>0.348776</td>\n","      <td>0.880712</td>\n","      <td>0.881493</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='180' max='180' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [180/180 00:20]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Fold 2 - Validation Accuracy: 0.8741, Validation F1: 0.8744\n","Training on Fold 3\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='540' max='2151' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 540/2151 03:44 < 11:12, 2.39 it/s, Epoch 0.75/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>0.468400</td>\n","      <td>0.320498</td>\n","      <td>0.864318</td>\n","      <td>0.864716</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["from sklearn.model_selection import StratifiedKFold\n","\n","# Combine the train and validate sets (texts and labels)\n","train_val_texts = train_texts + val_texts\n","train_val_labels = train_labels + val_labels\n","\n","# Initialize StratifiedKFold\n","num_splits = 5\n","skf = StratifiedKFold(n_splits=num_splits, shuffle=True, random_state=42)\n","\n","# Create the k different training and validation splits\n","folds = []\n","for train_index, val_index in skf.split(train_val_texts, train_val_labels):\n","    train_texts_fold = [train_val_texts[i] for i in train_index]\n","    val_texts_fold = [train_val_texts[i] for i in val_index]\n","    train_labels_fold = [train_val_labels[i] for i in train_index]\n","    val_labels_fold = [train_val_labels[i] for i in val_index]\n","\n","    folds.append((train_texts_fold, train_labels_fold, val_texts_fold, val_labels_fold))\n","\n","# Iterate over the k folds\n","for fold_idx, (train_texts_fold, train_labels_fold, val_texts_fold, val_labels_fold) in enumerate(folds):\n","    print(f\"Training on Fold {fold_idx + 1}\")\n","\n","    # Create the training and validation datasets\n","    train_dataset_fold = HateSpeechDataset(train_texts_fold, train_labels_fold, tokenizer, max_len)\n","    val_dataset_fold = HateSpeechDataset(val_texts_fold, val_labels_fold, tokenizer, max_len)\n","\n","    # Reinitialize the model for each fold\n","    model = AutoModelForSequenceClassification.from_pretrained(model_name)\n","\n","    # Create the Trainer\n","    trainer = Trainer(\n","        model=model,\n","        args=training_args,\n","        train_dataset=train_dataset_fold,\n","        eval_dataset=val_dataset_fold,\n","        compute_metrics=compute_metrics,\n","        callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],\n","    )\n","\n","    # Train the model on the current fold\n","    trainer.train()\n","\n","    # Evaluate on the current fold\n","    eval_result = trainer.evaluate(val_dataset_fold)\n","    print(f\"Fold {fold_idx + 1} - Validation Accuracy: {eval_result['eval_accuracy']:.4f}, Validation F1: {eval_result['eval_f1']:.4f}\")\n","\n","# Step 6: Evaluate on Test Set\n","eval_result = trainer.evaluate(test_dataset)\n","print(f\"Test Accuracy: {eval_result['eval_accuracy']:.4f}, Test F1: {eval_result['eval_f1']:.4f}\")\n","\n","# Get the predicted labels for the test dataset\n","test_predictions = trainer.predict(test_dataset).predictions\n","test_predicted_labels = np.argmax(test_predictions, axis=1)\n","\n","# Create a dictionary to store the collected information\n","test_data_dict = {\n","    \"original_sentence\": test_texts,  # Original sentences from the test dataset\n","    \"hate_label\": test_labels,  # Ground truth hate labels from the HateSpeechDataset\n","    \"predicted_label\": test_predicted_labels,  # Predicted labels from the model\n","}\n","\n","# Create a DataFrame from the dictionary\n","test_results_df = pd.DataFrame(test_data_dict)"]},{"cell_type":"markdown","id":"4b14705d","metadata":{"id":"4b14705d"},"source":["Producing the dataframe of the test sentences, their ground truth labels and the predicted labels from the model."]},{"cell_type":"code","execution_count":null,"id":"f5e091d2","metadata":{"id":"f5e091d2"},"outputs":[],"source":["# Inner merge based on the condition where 'sentences' matches 'original_sentence'\n","merged_df = data.merge(test_results_df, left_on='sentences', right_on='original_sentence', how='inner')\n","\n","# Drop the duplicate 'original_sentence' column after the merge\n","merged_df.drop('original_sentence', axis=1, inplace=True)"]},{"cell_type":"code","execution_count":null,"id":"18c2fa22","metadata":{"id":"18c2fa22"},"outputs":[],"source":["columns_to_drop = ['hate_label_y']\n","merged_df = merged_df.drop(columns_to_drop, axis=1)"]},{"cell_type":"code","execution_count":null,"id":"c67c2470","metadata":{"id":"c67c2470"},"outputs":[],"source":["column_name_mapping = {\n","    'hate_label_x': 'hate_label',\n","}\n","\n","# Rename the columns using the dictionary\n","merged_df.rename(columns=column_name_mapping, inplace=True)"]},{"cell_type":"code","execution_count":null,"id":"784a5105","metadata":{"id":"784a5105"},"outputs":[],"source":["columns_to_drop = ['clean_sentences']\n","merged_df = merged_df.drop(columns_to_drop, axis=1)"]},{"cell_type":"markdown","id":"a418ca14","metadata":{"id":"a418ca14"},"source":["Downloading the CSV file"]},{"cell_type":"code","execution_count":null,"id":"7cdb7339","metadata":{"id":"7cdb7339"},"outputs":[],"source":["from google.colab import files\n","\n","merged_df.to_csv(\"2.55_finetune_test_dataset_analysis\", index=False)\n","files.download(\"2.55_finetune_test_dataset_analysis\")"]},{"cell_type":"code","source":["Uploading the model to the Huggingface Hub"],"metadata":{"id":"0bKuQ3LP0mtg"},"id":"0bKuQ3LP0mtg","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"149eb02d","metadata":{"id":"149eb02d"},"outputs":[],"source":["# model_path = \"EZiisk/EZ_finetune_Vidgen_model_RHS\"\n","# save_path = \"EZiisk/EZ_finetune_Vidgen_model_RHS_tokenizer\"\n","# model.save_pretrained(model_path, push_to_hub = True)\n","# tokenizer.save_pretrained(save_path, push_to_hub = True)"]}],"metadata":{"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}